---
title: "CITS5507 Project 2: Parallel implementation of search based on MPI and OpenMP"

graphics: yes
author: Kaiqi Liang (23344153), Briana Davies-Morrell (22734723)
date: "Semester 2, 2023"
---

```{r, message=FALSE, warning=FALSE, echo=FALSE}
library(ggplot2)
library(knitr)
```

# Introduction

Fish School Behaviour (FSB) is a heuristic algorithm used for multi-dimensional optimization problems. The objective of this project was to parallelise a much simpler version of the FSB algorithm using both MPI (Message Passing Interface) and OpenMP (Open Multi-Processing) frameworks. The goal was to build upon our previous OpenMP implementation from Project 1 and integrate MPI for distributed computing across multiple nodes.

The project consists of two main deliverables; the actual MPI implementation and experiments with different configurations, including varying the number of MPI nodes and the number of threads under each MPI node.

# Implementation

All of the code can be found on [**GitHub**](https://github.com/Kaiqi-Liang/CITS5507-Project).

## File Writing

In this section, we established effective communication using MPI by performing an experiment in fish data distribution. The master process generates the fish data and distributes it evenly among the worker nodes. Each node, including the master, sends back its portion of the data to the master. This process is logged in two files before & after for validation.

The code for experimenting the communication using MPI by generating the fish data and outputting it to two files is in [`file_writing.cpp`](https://github.com/Kaiqi-Liang/CITS5507-Project/blob/main/file_writing.cpp). Submit job by running the following command.

```bash
sbatch file_writing.sh
```

After the job has completed there should be 2 files `before` and `after` which can be checked by running the following command.

```bash
diff before after
```

This will produce no output. The file comparison ensures that the transmitted data remains consistent across processes. In the case that the file_writing file is submitted with very large hyperparameters (such as 60 million fish), the two files will not be identical as only some of the data will be written to the files. This is because very large hyperparameters require storage in Setonix that we do not have access to.  

## Simulation

In this section, we integrated our OpenMP implementation from Project 1 with our new MPI implementation to simulate fish school behaviour. MPI serves as the overarching framework, while OpenMP operates beneath it, managing the parallelism at the thread level.

The code for programming multicore clusters to run the fish school simulation is in [`parallel.cpp`](https://github.com/Kaiqi-Liang/CITS5507-Project/blob/main/parallel.cpp). To use only MPI implementation run the following command.

```bash
sbatch mpi.sh
```

To use both MPI and OpenMP implementation run the following command.

```bash
sbatch parallel.sh
```

# Experiments

We experimented by varying the number of MPI nodes and adjust the number of threads under each MPI node. All the results have been measured several times and taken the average to get a better accuracy. 

In Project 1 the relationship between the hyperparamaters (fish & number of iterations) and execution time was established. No interaction was found between the number of threads and the hyperparameters (that is, the execution time increased linearly with either the fish or iterations, or quadratically with both, regardless of the number of threads in use). As such, hyperparameters were not experimented with this time around. Instead a constant value of 60,000,000 fish and 1000 iterations were used throughout all experiments. 

Moreover, Project 1 revealed the sequential implemented to have a faster execution time than any of the OpenMP implementations. As such, the sequential time is used as a baseline for comparison throughout all experiments performed here. For the set hyperparameters, the sequential time was 1755 seconds (~29 minutes).

## MPI & OpenMP

In our first experiment, we aimed to assess the impact of utilizing the MPI framework and varying the number of OpenMP threads under the MPI implementation on the execution time of the parallelised FSB algorithm. This involved timing the sequential implementation, the MPI only implementation wherein no OpenMP threads were involved as well as the OpenMP implementation under MPI with different number of threads.

The graph below shows the measured execution time using different number of MPI Nodes (1, 2, 3, 4), the baseline time of the sequential implementation and using different number of OpenMP threads including one implementation that did not use OpenMP (blue). As expected, the execution time decreases exponentially as the number of MPI nodes increases. In particular, one node has the worst execution of 1822 seconds (~30 minutes) which is slower than the sequential implementation. This makes sense, as the overheads from inter-process communication and synchronization outweigh the benefits of parallelism when there's only one node involved.

As we move to 2 nodes, the execution time drops significantly to 914 seconds (~15 minutes). With 3 nodes, the execution time further decreases to 613 seconds (~10 minutes), and with 4 nodes, it drops to 461 seconds (~8 minutes). These reductions in execution time demonstrate the effectiveness of parallel processing, with the workload being distributed and processed concurrently across multiple nodes.

It's important to note that while the execution time decreases with additional nodes, there is a diminishing return in performance improvement. This aligns with Amdahl's Law, which states that the speedup of a parallel program is limited by its sequential portion. In our case, as we increase the number of nodes, the sequential portion becomes a smaller fraction of the total execution time, leading to diminishing returns in speedup.

Looking at lines that are meant to plot the run time using different number of OpenMP threads we can only see the pink line as the execution times remain relatively stable across various configurations of OpenMP threads under each MPI node. This trend suggests that the number of OpenMP threads does not impact the execution time.

However, compare to the MPI implementation that did not use OpenMP it is evident below that introducing OpenMP threads under each MPI node still improved the execution times by just above 10 seconds. This is due to the increased intra-node parallelism brought by OpenMP. The impact of OpenMP threads is most pronounced when there are fewer MPI nodes, as demonstrated by the steeper decrease in execution times. This signifies that within a single node, threading enables effective concurrent processing. However, the curve predominantly follows an exponential decrease as the number of MPI nodes increases, indicating that the MPI framework remains the primary driver of performance gains in this parallelised implementation.

```{r, message=FALSE, warning=FALSE, echo=FALSE}
ggplot(
  data.frame(
    num_nodes = seq(1, 4),
    sequential = 1754.656590,
    mpi = c(1822.379590476, 914.486197804, 612.819364970, 460.707667057),
    threads_2 = c(1770.137878338, 888.847948670, 595.635013834, 449.641916923),
    threads_4 = c(1769.391758150, 888.820532438, 594.955746320, 448.248664594),
    threads_8 = c(1771.727156207, 891.127868899, 595.044071098, 449.024061848),
    threads_16 = c(1770.336729298, 889.954078117, 596.349251760, 449.729564190)
  ),
  aes(x = num_nodes)) +
  labs(title = "Execution Time vs Number of MPI Nodes",
    x = "Number of Nodes",
    y = "Time (s)",
    color = "Number of Threads") +
  geom_line(aes(y = sequential, color = "1 (Sequential)"),
    linetype = "dashed") +
  geom_smooth(aes(y = mpi, color = "1 (MPI Only)"),
    method = "lm", formula = y ~ exp(-x), se = FALSE) +
  geom_point(aes(y = mpi, color = "1 (MPI Only)"), shape = 21) +
  geom_smooth(aes(y = threads_2, color = "2"),
    method = "lm", formula = y ~ exp(-x), se = FALSE) +
  geom_point(aes(y = threads_2, color = "2"), shape = 22) +
  geom_smooth(aes(y = threads_4, color = "4"),
    method = "lm", formula = y ~ exp(-x), se = FALSE) +
  geom_point(aes(y = threads_4, color = "4"), shape = 23) +
  geom_smooth(aes(y = threads_8, color = "8"),
    method = "lm", formula = y ~ exp(-x), se = FALSE) +
  geom_point(aes(y = threads_8, color = "8"), shape = 24) +
  geom_smooth(aes(y = threads_16, color = "16"),
    method = "lm", formula = y ~ exp(-x), se = FALSE) +
  geom_point(aes(y = threads_16, color = "16"), shape = 25) +
  theme(panel.grid.minor = element_blank()) +
  scale_color_manual(values = c(
    "1 (Sequential)" = "purple",
    "1 (MPI Only)" = "steelblue",
    "2" = "darkgreen",
    "4" = "darkred",
    "8" = "magenta",
    "16" = "pink"))
```

### Number of Tasks Per Node

In our second experiment, we investigated the impact of varying the number of tasks per nodes for 4 tasks (by setting `--ntasks=4`) under the MPI and combined with OpenMP implementation on the execution time of the parallelised FSB algorithm.

We fixed the number of tasks to 4 and experimented with 1, 2, 3, 4 and 5 number of tasks per nodes which resulted in being allocated 4, 2 and 1 Setonix compute node. If we specified 3 number of tasks per node it will print the following because 4 is not divisible by 3.

```
Warning: can't honor --ntasks-per-node set to 3 which doesn't match the requested tasks 4
with the number of requested nodes 2. Ignoring --ntasks-per-node.
```

Our hypothesis is any number from 3 will all have the same result because we only have 4 tasks, 3 tasks per node is ignored and any number greater than or equal to 4 will get allocated 1 node to run all 4 tasks. The graph below confirmed our hypothesis, the last 3 data points (3, 4 and 5 number of tasks per node) for each line stay flat. It also shows that assigning 1 task per node gives the best performance because we get 4 compute nodes rather than sharing the CPUs within a node between task. However when putting all tasks into 1 node completely avoided the communication between nodes via network connections, thus giving us only slightly better speedup than having 2 nodes with 2 tasks each allocated to each, but still not as good as the pure performance gain from using completely separate nodes for each task. This trend is true across both MPI only implementation and combined with OpenMP for all number of threads, and to echo the conclusion from first experiment most OpenMP experiments turned out to be slightly faster than without using OpenMP but roughly the same time regardless of the number of threads used.

```{r, message=FALSE, warning=FALSE, echo=FALSE}
ggplot(
  data = data.frame(
    num_tasks = seq(1, 5),
    mpi = c(
      460.864967726,
      470.456927876,
      464.579775953,
      464.380644771,
      464.408472428),
    thread_2 = c(
      452.171921260,
      463.303976195,
      459.320273628,
      458.472190107,
      458.456342535),
    thread_4 = c(
      457.904325192,
      465.062157786,
      463.172169014,
      463.488311895,
      463.548838141),
    thread_8 = c(
      451.987779002,
      462.307877353,
      460.291607817,
      459.908851021,
      459.808448797),
    thread_16 = c(
      452.892428487,
      464.530417067,
      460.791890543,
      459.940209202,
      459.792117470)
  ),
  aes(x = num_tasks)) +
  geom_line(aes(y = mpi, color = "1 (MPI Only)")) +
  geom_point(aes(y = mpi), shape = 21) +
  geom_line(aes(y = thread_2, color = "2")) +
  geom_point(aes(y = thread_2), shape = 22) +
  geom_line(aes(y = thread_4, color = "4")) +
  geom_point(aes(y = thread_4), shape = 23) +
  geom_line(aes(y = thread_8, color = "8")) +
  geom_point(aes(y = thread_8), shape = 24) +
  geom_line(aes(y = thread_16, color = "16")) +
  geom_point(aes(y = thread_16), shape = 25) +
  labs(title = "Execution Time vs Number of Tasks Per Node for 4 Tasks",
    x = "Number of Tasks Per Node",
    y = "Time (s)",
    color = "Number of Threads") +
  scale_color_manual(values = c(
    "1 (MPI Only)" = "steelblue",
    "2" = "darkgreen",
    "4" = "darkred",
    "8" = "magenta",
    "16" = "pink"))
```
