---
title: "CITS5507 Project 1: Parallel implementation of search based on Fish School Behaviour"

graphics: yes
author: Kaiqi Liang (23344153), Briana Davies-Morrell (22734723)
date: "Semester 2, 2023"

output:
  html_document:
    number_sections: true
---

```{r, message=FALSE, warning=FALSE, echo=FALSE}
library(ggplot2)
```

# Introduction
Fish School Behaviour (FSB) is a heuristic algorithm used for multi-dimensional optimization problems. The objective of this project was to parallelize a simple version of the FSB algorithm and perform a range of experiments assessing the impact of changing the hyperparameters and parallelisation strategies on execution time.

## Sequential Implementation & Experiments 

To establish a baseline for comparison, a sequential C program was developed to simulate the behaviour of a fish school using the FSB algorithm. 

The graph below visualizes the relationship between the number of fish and the execution time (in seconds) for a different number of iterations in the FSB algorithm. Increasing the number of fish, for a set number of iterations, or increasing the number of iterations, for a set number of fish, results in a linear increase in execution time. Increasing both the number of fish and iterations results in a parabolic increase in execution time. This behaviour is to be expected as the FSB algorithm consists of two nested loops over the number of iterations and fish and thus has a time complexity of O(FI) where F is the number of fish and I is the number of iterations. Overall, a larger population of fish and more iterations require more computation, resulting in longer execution times.

Our most computationally expensive experiment ran for 2780 seconds (~46 minutes) at 1,000,000 fish and 100,000 iterations. This value was used as a baseline for comparison for all parallel experiments. 

```{r, message=FALSE, warning=FALSE, echo=FALSE}
ggplot(
  data = data.frame(
    num_fish =  c(200000, 400000, 600000, 800000, 1000000),
    iter_20000 = c(110.703507, 221.168173, 332.295848, 443.077424, 557.648611),
    iter_40000 = c(222.853256, 448.680679, 677.698978, 886.666866, 1112.435153),
    iter_60000 = c(
      334.170646,
      666.771699,
      1004.9347861,
      1455.1160081,
      1820.057278),
    iter_80000 = c(
      442.774348,
      887.792977,
      1337.221277,
      1789.476277,
      2263.432539),
    iter_100000 = c(
      575.766097,
      1126.112223,
      1697.037298,
      2230.462189,
      2780.588561)),
  aes(x = num_fish)) +
  geom_smooth(aes(y = iter_20000, color = "20000"), method = "lm", se = FALSE) +
  geom_smooth(aes(y = iter_40000, color = "40000"), method = "lm", se = FALSE) +
  geom_smooth(aes(y = iter_60000, color = "60000"), method = "lm", se = FALSE) +
  geom_smooth(aes(y = iter_80000, color = "80000"), method = "lm", se = FALSE) +
  geom_smooth(aes(y = iter_100000, color = "100000"), method = "lm",
    se = FALSE) +
  geom_point(aes(y = iter_20000)) +
  geom_point(aes(y = iter_40000)) +
  geom_point(aes(y = iter_60000)) +
  geom_point(aes(y = iter_80000)) +
  geom_point(aes(y = iter_100000)) +
  geom_smooth(aes(y = c(
    110.703507,
    448.680679,
    1004.9347861,
    1789.476277,
    2780.588561)),
    method = "lm",
    formula = y ~ poly(x, 2),
    linetype = "dotted",
    se = FALSE) +
  labs(title = "Execution Time vs Hyperparameters for Sequential Program",
    x = "Number of Fish",
    y = "Time (s)",
    color = "Number of Iterations") +
  scale_color_manual(values = c(
    "20000" = "darkred",
    "40000" = "steelblue",
    "60000" = "magenta",
    "80000" = "darkgreen",
    "100000" = "pink4"))
```

## Parallel Implementation & Experiments

A parallel implementation of the FSB algorithm was developed using OpenMP. We explored the following aspects of the parallelization process on execution time:

1. Number of Threads

2. Hyperparameters: Number of Fish & Iterations

3. Thread Scheduling

4. Variety of OMP Directives

It is worth noting that while the cache behaviour of Setonix was researched, we came to the conclusion that it was not possible to perform conclusive experiments as the closed source nature of Paswey means we do not know what chunk size is used. Thus, we have excluded this from our report.

### Number of Threads

In our first experiment, we aimed to assess the impact of the number of threads on the parallelized FSB algorithm's performance while keeping hyperparameters and thread scheduling constant. Specifically, we maintained the number of fish at 1,000,000 and the number of iterations at 100,000 so that the results could be compared against the sequential implementation. Additionally, the default scheduling strategy of OpenMP (static) was employed with the default chunk size (unspecified).

The graph below visualizes the relationship between the number of threads in the parallel program and the execution time (in seconds). For the purpose of comparison the sequential time is also indicated on the graph as a dashed line. Comparing the sequential and parallel implementations, no speedup was achieved. Rather, the sequential program had the fastest execution time of 2780 seconds (~46 minutes), likely due to the absence of thread management overhead. 

For the parallel implementation, a low number of threads (1-3) resulted in the slowest execution time which indicates that the overhead of thread management significantly outweighs the returns achieved from parallelizing the program. Some speedup was achieved in increasing the number of threads from 3 to 4, with a local minimum of 2925 seconds (~48 minutes) achieved at 4 threads, thus indicating an optimal balance between thread overheads and returns from parallelization. However, beyond 4 threads the execution time increased before remaining relatively stable at 3000 seconds (~50 minutes). The diminishing returns in terms of parallelization efficiency can be attributed to increased overhead from thread management.

```{r, message=FALSE, warning=FALSE, echo=FALSE}
ggplot(
    data.frame(
      num_threads = c(1, 2, 3, 4, 5, 6, 8, 16, 32),
      parallel = c(
        3220.587380,
        3471.353496,
        2984.539602,
        2925.219894,
        2944.861335,
        2977.993479,
        2976.687371,
        3000.694720,
        3028.914719),
      sequential = 2780.588561),
    aes(x = num_threads)) +
    labs(title = "Execution Time vs Number of Threads",
      x = "Number of Threads",
      y = "Time (s)",
      linetype = "Parallel / Sequential") +
    geom_smooth(aes(y = sequential, linetype = "sequential")) +
    geom_line(aes(y = parallel, linetype = "parallel")) +
    geom_point(aes(y = parallel)) +
    scale_y_continuous(breaks = c(
      seq(2900, 3500, by = 100), 2780)) +
    scale_x_continuous(breaks = c(
      1:8,
      seq(9, 15, by = 2),
      seq(17, 32, by = 3))) +
    theme(panel.grid.minor = element_blank())

```

### Hyperparameters: Number of Fish & Iterations

In the second experiment, we assessed the impact of varying two critical hyperparameters, the number of fish and the number of iterations, on the execution time of the parallel FSB algorithm. The number of threads was fixed at 4 as this was proved to provide an optimal execution time, as demonstrated in the previous experiment. Similar to the first experiment, static scheduling was employed.

Similar to the graph provided for the sequential implementation, the graph below visualizes the relationship between the number of fish and the execution time (in seconds) for a different number of iterations in the FSB algorithm. The results are akin to those found for the sequential code - increasing one hyperparameter results in a linear increase in execution time and increasing both hyperparameters results in a parabolic increase in execution time. Overall, all execution times are slower when compared to the sequential version. 

```{r, message=FALSE, warning=FALSE, echo=FALSE}
ggplot(
  data = data.frame(
    num_fish =  c(200000, 400000, 600000, 800000, 1000000),
    iter_20000 = c(117.350819, 237.082449, 397.935612, 482.203047, 653.402037),
    iter_40000 = c(234.463991, 466.925614, 700.021926, 936.720754, 1177.662832),
    iter_60000 = c(
      353.012268,
      701.901361,
      1050.975947,
      1418.876797,
      1777.164701),
    iter_80000 = c(
      471.402103,
      944.524666,
      1397.776130,
      1863.592248,
      2350.380621),
    iter_100000 = c(
      586.554482,
      1167.285182,
      1771.424057,
      2363.592248,
      2925.219894)),
  aes(x = num_fish)) +
  geom_smooth(aes(y = iter_20000, color = "20000"), method = "lm", se = FALSE) +
  geom_smooth(aes(y = iter_40000, color = "40000"), method = "lm", se = FALSE) +
  geom_smooth(aes(y = iter_60000, color = "60000"), method = "lm", se = FALSE) +
  geom_smooth(aes(y = iter_80000, color = "80000"), method = "lm", se = FALSE) +
  geom_smooth(aes(y = iter_100000, color = "100000"), method = "lm",
    se = FALSE) +
  geom_point(aes(y = iter_20000)) +
  geom_point(aes(y = iter_40000)) +
  geom_point(aes(y = iter_60000)) +
  geom_point(aes(y = iter_80000)) +
  geom_point(aes(y = iter_100000)) +
  geom_smooth(aes(y = c(
    117.350819,
    466.925614,
    1050.975947,
    1863.592248,
    2925.219894)),
    method = "lm",
    formula = y ~ poly(x, 2),
    linetype = "dotted",
    se = FALSE) +
  labs(title = "Execution Time vs Hyperparameters for Parallel Program",
    x = "Number of Fish",
    y = "Time (s)",
    color = "Number of Iterations") +
  scale_color_manual(values = c(
    "20000" = "darkred",
    "40000" = "steelblue",
    "60000" = "magenta",
    "80000" = "darkgreen",
    "100000" = "pink4"))
```

### Thread Scheduling

In our third experiment, we investigated the impact of different thread scheduling strategies (static, dynamic, guided, and runtime) on the execution time of the parallelized FSB algorithm. Similar to previous experiments, the number of fish and iterations was fixed at 1,000,000 and 100,000 respectively. Additionally, the number of threads was fixed at 4 as this was proved to provide an optimal execution time.

The graph below visualizes the relationship between the chunk size in the parallel program and the execution time (in seconds), across different scheduling strategies. While runtime scheduling does not employ a fixed chunk size, instead adjusting the value during runtime, it's recorded execution time is indicated on the graph as a dashed line for the purposes of comparison with the other scheduling types

* add comparison between different strategies and different chunk sizes within each stratgey 
* also compare with sequential baseline

As the time limit on Setonix is 1 hour, anything over 3600 seconds were run with fewer number of steps and scale the time back to the same as every other experiment, based on the previous experiments on Execution Time vs Hyperparameters we can conclude that execution time grows linerally with the nubmer of steps, so this estimate is accurate.
* maybe explain a bit more how this was done - i don't really understand 

Runtime scheduling is the slowest as there is a lot of overhead monitoring the performance during runtime while adjusting the chunk size. Dynamic scheduling with chunk size of 1 means each thread grabs 1 iteration off a queue each time which does not require much work so the overhead of too many thread switching makes is also slow. Static scheduling with a chunk size of 10 seems to be the optimal amount of work to allocate to each thread as it has the best performance. The rest scheduling methods and chunk sizes result in roughly the same performance.

```{r, message=FALSE, warning=FALSE, echo=FALSE}
ggplot(
  data = data.frame(
    chunk_size = c(1, 2, 3, 4),
    static = c(3265.162540, 2784.864581, 3035.950782, 2918.142244),
    dynamic = c(4154.18010, 3073.691588, 2970.014433, 2982.671884),
    guided = c(3051.865076, 2937.002249, 2941.794308, 2973.637886),
    runtime = 4198.46404),
  aes(x = chunk_size)) +
  geom_smooth(aes(y = static, color = "static")) +
  geom_smooth(aes(y = dynamic, color = "dynamic")) +
  geom_smooth(aes(y = guided, color = "guided")) +
  geom_smooth(aes(y = runtime, color = "runtime"), linetype = "dashed") +
  geom_point(aes(y = static)) +
  geom_point(aes(y = dynamic)) +
  geom_point(aes(y = guided)) +
  labs(title = "Execution Time vs Scheduling Strategies",
    x = "Chunk Size",
    y = "Time (s)",
    color = "Scheduling Type") +
  scale_x_continuous(labels = c(1, 10, 100, 10000)) +
  scale_y_continuous(breaks = seq(2700, 4200, by = 300)) +
  scale_color_manual(values = c(
    "static" = "darkred",
    "dynamic" = "steelblue",
    "guided" = "magenta",
    "runtime" = "darkgreen"))
```

### OMP Directives

In our final experiment, we explored a range of paraellization strategies and OMP directives. Similar to previous experiments, the number of fish and iterations was fixed at 1,000,000 and 100,000 respectively, the number of threads was fixed at 4 and the scheduling type was static. 
* checking these were the values you used?

Four different implementations of our parallel programs were explored. OMP For is simply using reduction of all 3 for loops (finding the maximum difference, for every fish eat and then swim, calculate the barycentre). OMP For Partition sorts the fish based on their coordinates in every step, so that OMP For can split the fish based on where they are in the pool acting like partitioning the whole pool into smaller regions and assign threads that way, but this turns out to be just extra work with no benefits. OMP Task Per Fish turns each fish's action in the second for loop into a task, this is a magnitude slower than the other implementations because the action is almost a constant number of operations, which is much faster than the overhead of thread switching. OMP Tasks splits the calculation for barycentre into 2, one is the denominator and the other is the numerator, these are independent but large amount of calculations so 2 threads will be assigned pick up these 2 tasks, it is slightly slower than the OMP For implementation as we are no longer splitting the work amongst the larger number of fish. Therefore in this situation OMP For is the best implementation as it makes the most sense to assign threads to handle a smaller number of fish and the position of the fish has no impact on the calculation.
* maybe break into a few paragraphs. first explaining what the strategies are, then a final para just to compare? so its easier to underatand 

```{r, message=FALSE, warning=FALSE, echo=FALSE}
ggplot(
  data = data.frame(
    omp_directive = c(
      "OMP For",
      "OMP For Partition",
      "OMP Task Per Fish",
      "OMP Tasks"),
    time = c(
      295.652146,
      885.287894,
      4877.38059,
      305.464913),
    colour_group = c(rep("OMP For", 2), rep("OMP Task", 2))),
    aes(x = omp_directive, y = time, fill = colour_group)) +
  geom_bar(stat = "identity") +
  labs(title = "Execution Time for Different Implementations",
    x = "Implementations",
    y = "Time (s)",
    fill = "OMP Directives"
  )
```
